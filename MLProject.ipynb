{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRjm9wyZZHuH"
   },
   "source": [
    "## Machine Learning Project 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYNFp1lAdiwX"
   },
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:48:20.413662Z",
     "start_time": "2024-01-02T14:48:19.326113Z"
    },
    "id": "0spGcCdEZW3S"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.linalg import eigh\n",
    "import seaborn as sns\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, mean_squared_error,roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DkfpGwUZZsj"
   },
   "source": [
    "### Preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:48:23.252641Z",
     "start_time": "2024-01-02T14:48:23.184005Z"
    },
    "id": "A-l8rfKNZeXF"
   },
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    train_dataset = np.load('fashion_train.npy')\n",
    "    test_dataset = np.load('fashion_test.npy')\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def split_datasets(train_data,test_data):\n",
    "    train_X = train_data[:, :-1]\n",
    "    train_y = train_data[:, -1]\n",
    "    test_X = test_data[:, :-1]\n",
    "    test_y = test_data[:, -1]\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "# Loading the dataset using our preprocessing script\n",
    "train_dataset, test_dataset = load_datasets()\n",
    "\n",
    "# Spliting the dataset\n",
    "X_train, y_train, X_test, y_test = split_datasets(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:48:32.538719Z",
     "start_time": "2024-01-02T14:48:31.611507Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5msGJ3Skdjmp",
    "outputId": "8283fce7-7a40-4569-d7fc-6c25224cf2b3"
   },
   "outputs": [],
   "source": [
    "def missing_values(*dataframe):\n",
    "    count = 0\n",
    "    for i in dataframe:\n",
    "        count += pd.DataFrame(i).isnull().sum().sum()\n",
    "    if count == 0:\n",
    "        return False\n",
    "    else: return True\n",
    "\n",
    "# Checking for missing values in training and test datasets\n",
    "if missing_values(test_dataset) == False:\n",
    "    print(\"There are no missing values in the test set.\")\n",
    "else:\n",
    "    print(\"There are missing values in the test set.\")\n",
    "\n",
    "if missing_values(train_dataset) == False:\n",
    "    print(\"There are no missing values in the training set.\")\n",
    "else:\n",
    "    print(\"There are missing values in the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:48:36.712020Z",
     "start_time": "2024-01-02T14:48:36.480570Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhTMmBzCdfwA",
    "outputId": "d20f58c0-dec1-4dd9-e08e-fcbc468241c1"
   },
   "outputs": [],
   "source": [
    "# Check the dataset size\n",
    "print(\"Training data shape:\", train_dataset.shape)\n",
    "print(\"Test data shape:\", test_dataset.shape)\n",
    "\n",
    "# Verify first few rows of training data\n",
    "print(train_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:48:46.473764Z",
     "start_time": "2024-01-02T14:48:42.720319Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "_Cdd8u8qdiwa",
    "outputId": "6c84ee98-4796-47e5-fb10-1187df1acbe6"
   },
   "outputs": [],
   "source": [
    "def class_distribution(train_y, test_y):\n",
    "    train, test = Counter(train_y), Counter(test_y)\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(12,4))\n",
    "    sns.barplot(x = [\"Tshirt/Top\", \"Trouser\",\"Pullover\", \"Dress\", \"Shirt\"], y = list(train.values()), palette = \"PuBu\", ax = ax[0])\n",
    "    sns.barplot(x = [\"Tshirt/Top\", \"Trouser\",\"Pullover\", \"Dress\", \"Shirt\"], y = list(test.values()), palette = \"PuBu\", ax = ax[1])\n",
    "    ax[0].set_title(\"Train split class distribution\")\n",
    "    ax[1].set_title(\"Test split class distribution\")\n",
    "\n",
    "# Check class distribution in both datasets\n",
    "class_distribution(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:48:47.282038Z",
     "start_time": "2024-01-02T14:48:46.251802Z"
    },
    "id": "WMeOcBDddiwa"
   },
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StSlpm_veEW7"
   },
   "source": [
    "### Visualization and exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISCYKSx8eLF0"
   },
   "source": [
    "#### PCA and LDA (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:49:03.097386Z",
     "start_time": "2024-01-02T14:48:58.918292Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "I9B-rFAMeQ_V",
    "outputId": "f667d10f-6790-48fb-a36e-732de1fe7783"
   },
   "outputs": [],
   "source": [
    "# PCA projeection using sklearn\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c=y_train, edgecolor='k')\n",
    "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Shirt']\n",
    "legend1 = plt.legend(handles=scatter.legend_elements()[0], labels=class_labels, title=\"Clothing Types\")\n",
    "\n",
    "plt.title('PCA Projection')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T14:49:08.565083Z",
     "start_time": "2024-01-02T14:49:03.096779Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "U55RkkoTgze1",
    "outputId": "eb0956fd-27d2-443d-f1a6-8d22e1d7f80f"
   },
   "outputs": [],
   "source": [
    "# LDA projeection using sklearn\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "\n",
    "X_lda_sklearn = lda.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "scatter = plt.scatter(X_lda_sklearn[:, 0], X_lda_sklearn[:, 1], c=y_train, edgecolor='k')\n",
    "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Shirt']\n",
    "legend1 = plt.legend(handles=scatter.legend_elements()[0], labels=class_labels, title=\"Clothing Types\")\n",
    "\n",
    "plt.title('LDA Projection')\n",
    "plt.xlabel('First Linear Discriminant')\n",
    "plt.ylabel('Second Linear Discriminant')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz6Aviczdiwc"
   },
   "source": [
    "#### LDA implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T16:21:43.347443Z",
     "start_time": "2024-01-02T16:21:41.914235Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "oTYbuSx0diwc",
    "outputId": "9eeb13d0-c7ea-4fe7-d4b7-4fac866a2eba"
   },
   "outputs": [],
   "source": [
    "# LDA projection from own function\n",
    "def lda_implementation(X, y):\n",
    "    # Compute class mean and class scatter matrix for each class\n",
    "    class_means = []\n",
    "    class_scatters = []\n",
    "    unique_classes = np.unique(y)\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        class_data = X[y == cls]\n",
    "        class_means.append(np.mean(class_data, axis=0))\n",
    "        class_scatters.append(np.cov(class_data.T))\n",
    "\n",
    "    # Sum the individual class scatter matrices\n",
    "    Sw = sum(class_scatters)\n",
    "\n",
    "    # Compute the mean\n",
    "    mean = np.mean(X, axis=0)\n",
    "\n",
    "    # Compute the between-class scatter matrix\n",
    "    Sb = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for i in range(len(unique_classes)):\n",
    "        ni = len(X[y == unique_classes[i]])\n",
    "        mean_diff = class_means[i] - mean\n",
    "        Sb += ni * np.outer(mean_diff, mean_diff)\n",
    "\n",
    "    # Solve the eigenvalue problem\n",
    "    eigenvalues, eigenvectors = eigh(Sb, Sw, subset_by_index=(X.shape[1] - 2, X.shape[1] - 1))\n",
    "\n",
    "\n",
    "# Sort eigenvectors by descending eigenvalues\n",
    "    sort_order = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvectors = eigenvectors[:, sort_order]\n",
    "\n",
    "    # Normalize the eigenvectors\n",
    "    eigenvectors_normalized = eigenvectors / np.linalg.norm(eigenvectors, axis=0)\n",
    "\n",
    "    # Project the data onto the linear discriminant space using normalized eigenvectors\n",
    "    X_lda = X.dot(eigenvectors_normalized)\n",
    "\n",
    "    # Return transformed data and the eigenvectors\n",
    "    return X_lda, eigenvectors_normalized\n",
    "\n",
    "# Call the LDA implementation function and capture the eigenvectors\n",
    "X_lda, eigenvectors_normalized = lda_implementation(X_train_scaled, y_train)\n",
    "\n",
    "# Using the eigenvectors to transform the test data\n",
    "X_lda_test = X_test_scaled.dot(eigenvectors_normalized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jj_t-jtVdo6-"
   },
   "source": [
    "### NaiveBayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T17:23:59.369253Z",
     "start_time": "2024-01-02T17:16:36.252141Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNSdZ62PeFQ6",
    "outputId": "7e997955-bea1-4731-9569-5a13407c35c4"
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "\n",
    "\t\"\"\"\tscore = (y_true - y_pred) / len(y_true) \"\"\"\n",
    "\n",
    "\treturn round(float(sum(y_pred == y_true))/float(len(y_true)) * 100 ,2)\n",
    "\n",
    "def gaussian_kernel(x, data_point, bandwidth):\n",
    "    return (1 / (bandwidth * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - data_point) / bandwidth) ** 2)\n",
    "\n",
    "def kernel_density_estimation(x, data, bandwidth):\n",
    "    return np.mean([gaussian_kernel(x, point, bandwidth) for point in data])\n",
    "\n",
    "class NaiveBayes:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            likelihoods: Likelihood of each feature per class\n",
    "            class_priors: Prior probabilities of classes\n",
    "            pred_priors: Prior probabilities of features\n",
    "            features: All features of the dataset\n",
    "        \"\"\"\n",
    "        self.features = list()\n",
    "        self.likelihoods = {}\n",
    "        self.class_priors = {}\n",
    "        self.pred_priors = {}\n",
    "\n",
    "        self.X_train = np.array\n",
    "        self.y_train = np.array\n",
    "        self.train_size = int\n",
    "        self.num_feats = int\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.features = list(range(X.shape[1]))\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.train_size = X.shape[0]\n",
    "        self.num_feats = X.shape[1]\n",
    "\n",
    "        for feature in self.features:\n",
    "            self.likelihoods[feature] = {}\n",
    "            self.pred_priors[feature] = {}\n",
    "\n",
    "            for feat_val in np.unique(self.X_train[:, feature]):\n",
    "                self.pred_priors[feature].update({feat_val: 0})\n",
    "\n",
    "                for outcome in np.unique(self.y_train):\n",
    "                    self.likelihoods[feature].update({(feat_val, outcome): 0})\n",
    "                    self.class_priors.update({outcome: 0})\n",
    "\n",
    "        self._calc_class_prior()\n",
    "        self._calc_likelihoods()\n",
    "        self._calc_predictor_prior()\n",
    "\n",
    "    def _calc_class_prior(self):\n",
    "        \"\"\" P(c) - Prior Class Probability \"\"\"\n",
    "        for outcome in np.unique(self.y_train):\n",
    "            outcome_count = np.sum(self.y_train == outcome)\n",
    "            self.class_priors[outcome] = outcome_count / self.train_size\n",
    "\n",
    "    def _calc_predictor_prior(self):\n",
    "        \"\"\" P(x) - Evidence \"\"\"\n",
    "        for feature in self.features:\n",
    "            feat_vals = dict(zip(*np.unique(self.X_train[:, feature], return_counts=True)))\n",
    "\n",
    "            for feat_val, count in feat_vals.items():\n",
    "                self.pred_priors[feature][feat_val] = count / self.train_size\n",
    "\n",
    "    def _calc_likelihoods(self):\n",
    "        for feature in self.features:\n",
    "            for outcome in np.unique(self.y_train):\n",
    "                outcome_data = self.X_train[self.y_train == outcome, feature]\n",
    "                bandwidth = 1.06 * np.std(outcome_data) * (len(outcome_data) ** (-1/5))\n",
    "                self.likelihoods[feature][outcome] = lambda x, d=outcome_data, b=bandwidth: kernel_density_estimation(x, d, b)\n",
    "\n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        X = np.array(X)\n",
    "        for query in X:\n",
    "            probs_outcome = {}\n",
    "            for outcome in np.unique(self.y_train):\n",
    "                prior = np.log(self.class_priors[outcome])\n",
    "                likelihood = 0  # Log likelihood of 0\n",
    "                for feat_idx, feat_val in enumerate(query):\n",
    "                    kde_func = self.likelihoods[feat_idx][outcome]\n",
    "                    likelihood += np.log(max(kde_func(feat_val), 1e-6))  # Avoid log(0) with max()\n",
    "                posterior = likelihood + prior\n",
    "                probs_outcome[outcome] = posterior\n",
    "            result = max(probs_outcome, key=probs_outcome.get)\n",
    "            results.append(result)\n",
    "        return np.array(results)\n",
    "\n",
    "# Naive Bayes model using LDA features\n",
    "nb_clf = NaiveBayes()\n",
    "nb_clf.fit(X_lda, y_train)\n",
    "\n",
    "# Predicting on LDA-transformed test data\n",
    "NB_predictions = nb_clf.predict(X_lda_test)\n",
    "\n",
    "NB_accuracy = accuracy_score(y_test, NB_predictions)\n",
    "print(f\"Accuracy: {NB_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T17:25:50.110695Z",
     "start_time": "2024-01-02T17:25:49.566618Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, NB_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix - Naive Bayes')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds0qJHoOdiwc"
   },
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05mLlUwcdiwc"
   },
   "source": [
    "We chose the XGBoost classifier for a multi-class classification task.\n",
    "\n",
    "The process involves hyperparameter tuning to optimize the model's performance.\n",
    "\n",
    "Implemented from: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEVZ6ydZdiwc"
   },
   "source": [
    "The `modelfit` function takes the XGBoost model, training dataset, list of feature column names and other parameters.\n",
    "\n",
    "It performs cross-validation for hyperparameter tuning and displays model metrics and feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T17:35:52.883172Z",
     "start_time": "2024-01-02T17:35:52.751639Z"
    },
    "id": "b7FPINDodiwc"
   },
   "outputs": [],
   "source": [
    "# Define a function to fit the XGBoost model\n",
    "def XGBoost(alg,                      # model instance\n",
    "            dtrain,                   # training dataset\n",
    "            predictors,               # list of feature column names\n",
    "            useTrainCV=True,          # cross-valuation for hyperparameter tuning\n",
    "            cv_folds=5,               # number of cross-validation folds\n",
    "            early_stopping_rounds=50  # early stopping rounds for training\n",
    "            ):\n",
    "    if useTrainCV:\n",
    "        # cross-validation to determine the optimal number of boosing rounds\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain.values, label=y_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds,verbose_eval=True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "    # fit the algorithm on the data\n",
    "    alg.fit(dtrain, y_train)\n",
    "\n",
    "    # predict and evaluate on the test set\n",
    "    dtrain_predictions = alg.predict(X_test_scaled)\n",
    "    dtrain_predprob = alg.predict_proba(X_test_scaled)[:,1]\n",
    "    dtrain_predprob_test = np.argmax(dtrain_predictions,axis=0)\n",
    "\n",
    "    # display model report\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(y_test, dtrain_predictions))\n",
    "\n",
    "    # plot feature importanace\n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_dK6L3Fdiwd"
   },
   "outputs": [],
   "source": [
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'min_child_weight': [1, 2, 3, 4, 5],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'objective': ['binary:logistic', 'multi:softmax'],\n",
    "    'num_class': [3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "# Initialize the base model and RandomizedSearchCV\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, n_iter=50, scoring='accuracy', n_jobs=-1, cv=5, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "# Use the best estimator for further predictions\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate the model\n",
    "final_pred_y = best_xgb_model.predict(X_test_scaled)\n",
    "final_predictions = [round(value) for value in final_pred_y]\n",
    "\n",
    "# Display final evaluation metrics\n",
    "print(\"Final Model Accuracy:\", accuracy_score(y_test, final_predictions))\n",
    "print(\"Final Model F1 Score:\", f1_score(y_test, final_pred_y, average='macro'))\n",
    "print(\"Final Model Recall Score:\", recall_score(y_test, final_pred_y, average='macro'))\n",
    "print(\"Final Model Mean Squared Error:\", mean_squared_error(y_test, final_pred_y))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, final_pred_y)\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix - XGBoost')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LNyi9L0diwd"
   },
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We chose the Multinomial Logistic Regression classifier for a multi-class classification task.\n",
    "\n",
    "The process involves hyperparameter tuning to optimize the model's performance.\n",
    "\n",
    "Implemented from #https://machinelearningmastery.com/multinomial-logistic-regression-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T17:31:19.028718Z",
     "start_time": "2024-01-02T17:28:26.809468Z"
    },
    "id": "7aBhJZSFdiwd"
   },
   "outputs": [],
   "source": [
    "# Multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=0.001, max_iter=5000)\n",
    "# Fitting the model on the training dataset\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model evaluation using cross-validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, random_state=1)\n",
    "n_scores = cross_val_score(model, X_train_scaled, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Model performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "# Making predictions\n",
    "MLR_pred = model.predict(X_test_scaled)\n",
    "# Evaluation of the predictions\n",
    "print(\"Accuracy:\", accuracy_score(y_test, MLR_pred))\n",
    "print(classification_report(y_test, MLR_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, MLR_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix - Logistic Regression (Multinomial)')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T23:05:42.240964Z",
     "start_time": "2023-12-28T19:36:45.829278Z"
    },
    "id": "Lc02RxfCdiwd"
   },
   "outputs": [],
   "source": [
    "# Tuning regularization for multinomial logistic regression\n",
    "penalties = [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "results, names = list(), list()\n",
    "for C_val in penalties:\n",
    "    # Defining model\n",
    "    if C_val == 0.0:\n",
    "        model = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty=None, max_iter=10000)\n",
    "    else:\n",
    "        model = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=C_val, max_iter=10000)\n",
    "    # Evaluation the model\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    results.append(scores)\n",
    "    names.append('C=%.4f' % C_val)\n",
    "    # Summarize\n",
    "    print('>C=%.4f, Mean Accuracy: %.3f (%.3f)' % (C_val, np.mean(scores), np.std(scores)))\n",
    "\n",
    "#Model performance for comparison\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(results, labels=names, showmeans=True)\n",
    "ax.set_title('Model Accuracy for Different C values')\n",
    "ax.set_xlabel('C Value')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n",
    "#https://machinelearningmastery.com/multinomial-logistic-regression-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for each model\n",
    "nb_accuracy = 72.84\n",
    "xgb_accuracy = 87.24\n",
    "mlr_accuracy = 83.7\n",
    "\n",
    "# Classifier names\n",
    "classifiers = ['Naive Bayes', 'XGBoost', 'Multinomial Logistic Regression']\n",
    "\n",
    "# Corresponding accuracies\n",
    "accuracies = [nb_accuracy, xgb_accuracy, mlr_accuracy]\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(classifiers, accuracies, color=['blue', 'green', 'orange'])\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Comparison of Classifier Accuracies')\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "# Displaying the values\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.5, f\"{v:.2f}%\", ha='center', va='bottom')\n",
    "\n",
    "# Plot\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
